{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0424cb64-4d4b-4e37-9b8d-4a7ba4e665ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7bff630-f826-40a5-9665-9c682a0e3909",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv('../data/data.csv',usecols=lambda x: x != \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f252a82d-a0ea-4e3d-873b-26ef61df3ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-07 14:43:31--  https://github.com/alexeygrigorev/minsearch/blob/main/minsearch/minsearch.py\n",
      "Resolving github.com (github.com)... 20.26.156.215\n",
      "Connecting to github.com (github.com)|20.26.156.215|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘minsearch.py.1’\n",
      "\n",
      "minsearch.py.1          [ <=>                ] 190.30K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2025-08-07 14:43:31 (99.3 MB/s) - ‘minsearch.py.1’ saved [194869]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/alexeygrigorev/minsearch/blob/main/minsearch/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400aa47d-2291-4d48-bbf5-13a1ed67c2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['exercise_name', 'type_of_activity', 'type_of_equipment', 'body_part',\n",
       "       'type', 'muscle_groups_activated', 'instructions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "962f47a8-5885-4dfb-8f78-d627531bb9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more: cannot open minserach.py: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!more minserach.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f4e0561-56ab-488b-9d8d-080c68a8d885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG_test.ipynb  minsearch.py  minsearch.py.1\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3623c9c-2447-4a0f-b502-c30a2018ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cddaf7ae-1879-4580-9be7-6a631ea05508",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-1.7B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe08585e-cb3e-41a3-82ce-f9a266bee591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6024a9257274b8aaca3b0cd3c4e9699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "539871a5-48e3-4432-b2b6-af4fe3d5c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0c210f7-86aa-4181-9f73-6d60e8d44db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29d2496b-a23b-4aff-b058-e6939e0f2f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Give me a short introduction to large language model.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba82c9be-ea5c-491e-8c98-92762bb07650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,    872,    198,  35127,    752,    264,   2805,  16800,    311,\n",
       "           3460,   4128,   1614,     13, 151645,    198, 151644,  77091,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9366d768-7c66-410c-8a7b-4fd8c54d0bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b072e8e7-f15b-41f1-843a-b17f1c154d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5b4b944-c36b-48f7-bbcf-a27b199dce7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, the user wants a short introduction to large language models. Let me start by defining what they are. Large language models are AI systems that can understand and generate human-like text. I should mention their key features like training on vast datasets, neural networks, and the ability to generate text on the fly.\n",
      "\n",
      "Wait, they might not know the specifics. I need to keep it simple. Maybe start with the basics: what they are, how they work, and their applications. Also, highlight their capabilities like language understanding, generation, and reasoning. Should I mention specific examples like GPT or BERT? Probably not necessary for a short intro. Focus on the general concept.\n",
      "\n",
      "Make sure to explain that they're trained on a lot of data, which allows them to learn patterns and generate coherent text. Also, note that they can be used in various fields like writing, coding, or customer service. Avoid jargon but keep it informative. Check for clarity and conciseness. Alright, that should cover the main points.\n",
      "</think>\n",
      "content: Large language models (LLMs) are advanced AI systems designed to understand, generate, and interact with human language. They are trained on vast datasets of text to learn patterns, syntax, and context, enabling them to produce coherent, context-aware responses. These models excel in tasks like writing, coding, answering questions, and more by leveraging neural networks to simulate human-like language processing. Their ability to adapt and learn continuously makes them versatile tools across industries, though they require significant computational resources and careful training to maintain accuracy and ethical standards.\n"
     ]
    }
   ],
   "source": [
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0624fa51-4a71-4fde-872c-458f8ca20c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
